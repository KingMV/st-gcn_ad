{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:77% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:77% !important; }</style>\"))\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from net.st_gcn import Model\n",
    "from feeder.feeder import Feeder\n",
    "from torchlight.io import IO\n",
    "np.set_printoptions(linewidth=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_args = {'layout':'openpose', 'strategy':'spatial'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading of original weights\n",
    "root_path = '/root/sharedfolder/Research/pose_ad/st-gcn/'\n",
    "io = IO(root_path)\n",
    "model_name = 'net.st_gcn.Model'\n",
    "model_args = {\n",
    "    'edge_importance_weighting':True,\n",
    "    'graph_args':graph_args,\n",
    "    'in_channels':3,\n",
    "    'num_class':400, }\n",
    "model_args_ft = {\n",
    "    'edge_importance_weighting':True,\n",
    "    'graph_args':graph_args,\n",
    "    'in_channels':3,\n",
    "    'num_class':4, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fio = io.load_model(model_name, **model_args)\n",
    "model_2ft = io.load_model(model_name, **model_args_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path =  os.path.join(root_path, 'models/kinetics-st_gcn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weights = torch.load(weights_path)\n",
    "weights_list = [[k.split('module.')[-1], v.cpu()] for k, v in weights.items()]\n",
    "weights = OrderedDict(weights_list)\n",
    "model_state_dict = model_2ft.state_dict()\n",
    "weights.pop('fcn.bias') # loading all but the Final FC layer's weight and bias\n",
    "weights.pop('fcn.weight')\n",
    "model_state_dict.update(weights)\n",
    "model_2ft.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_fcn_weights_str = ['fcn.weigth', 'fcn.bias']\n",
    "# model_fio = io.load_weights(model_fio, weights_path)\n",
    "# model_2ft = io.load_weights(model_2ft, weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_2ft.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Altered Model Definition\n",
    "new_class_num = 4\n",
    "\n",
    "# model_minus_last_list = list(model_2ft.children())[:-1]\n",
    "# model_minus_last_list.extend([nn.Conv2d(256, new_class_num, kernel_size=1)])\n",
    "# model_2ft = nn.Sequential(*model_minus_last_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing weights of the two models to verify removal of the last layer didn't harm the weights\n",
    "\n",
    "# orig_last_stgcn = model_fio.st_gcn_networks[9].parameters()\n",
    "# orig_weights_mat = torch.ones((1, 256))\n",
    "# for param in orig_last_stgcn:\n",
    "# #     print(param.data.shape)\n",
    "#     data256 = param.data.view((-1, 256))\n",
    "# #     print(data256.shape)\n",
    "#     orig_weights_mat = torch.cat((orig_weights_mat, data256), 0)\n",
    "# orig_weights_mat.shape\n",
    "\n",
    "# ft_last_stgcn = model_fio.st_gcn_networks[9].parameters()\n",
    "# ft_weights_mat = torch.ones((1, 256))\n",
    "# for param in ft_last_stgcn:\n",
    "# #     print(param.data.shape)\n",
    "#     data256 = param.data.view((-1, 256))\n",
    "# #     print(data256.shape)\n",
    "#     ft_weights_mat = torch.cat((ft_weights_mat, data256), 0)\n",
    "# ft_weights_mat.shape\n",
    "\n",
    "# ## Equivalence test\n",
    "# np.allclose(ft_weights_mat, orig_weights_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for the cel below\n",
    "def loader_initializer(feeder_args_dict, batch_size=32, suffle=True, num_workers=4, drop_last=True):\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=Feeder(**feeder_args_dict),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=suffle,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=drop_last)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_path = '/root/sharedfolder/datasets/data_ssd/kinetics-skeleton/st-gcn_kinetics/Kinetics/kinetics-skeleton/'\n",
    "train_data_path = os.path.join(data_dir_path, 'train_data.npy')\n",
    "train_label_path = os.path.join(data_dir_path, 'train_label.pkl')\n",
    "test_data_path = os.path.join(data_dir_path, 'val_data.npy')\n",
    "test_label_path = os.path.join(data_dir_path, 'val_label.pkl')\n",
    "\n",
    "test_feeder_args = {'data_path': test_data_path, 'label_path': test_label_path}\n",
    "train_feeder_args = {'data_path': train_data_path, 'label_path': train_label_path}\n",
    "\n",
    "batch_size=32\n",
    "data_loader = dict()\n",
    "data_loader['kinetics_train'] = loader_initializer(train_feeder_args, batch_size=batch_size)\n",
    "data_loader['kinetics_test' ] = loader_initializer(test_feeder_args, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_npy = np.load(train_data_path, mmap_mode='r')\n",
    "test_npy = np.load(test_data_path, mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_files = False\n",
    "gen_train = False\n",
    "if gen_train:\n",
    "    split_npy = train_npy\n",
    "    split_2gen = 'normal_train'\n",
    "    f = train_label_path\n",
    "else:\n",
    "    split_npy = test_npy\n",
    "    split_2gen = 'normal_test'\n",
    "    f = test_label_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = dict()\n",
    "label_path = dict()\n",
    "split_indices = dict()\n",
    "data_npy = dict()\n",
    "data_pkl = dict()  # An array of [sample_name, label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Split Generator\n",
    "data_pkl['normal_train'] = [[],[]]  # Important to zero so not to accumulate older values\n",
    "data_pkl['normal_test'] = [[],[]]\n",
    "for split in ['front_raises134', 'deadlifting88', 'clean_jerk59', 'forcast254', 'normal_train', 'normal_test', 'abnormal_test']:\n",
    "    data_path[split]  =  os.path.join(data_dir_path, 'ad_experiment', split+'_data.npy')\n",
    "    label_path[split] =  os.path.join(data_dir_path, 'ad_experiment', split+'_label.pkl')\n",
    "    split_num = re.findall('\\d+', split)\n",
    "    if split_num != [] and save_files:\n",
    "        split_indices[split] = [ind for ind, val in enumerate(label) if val == int(split_num[0])]\n",
    "        data_npy[split] = split_npy[split_indices[split]]\n",
    "        data_pkl[split] = [[sample_name[i] for i in split_indices[split]], [label[i] for i in split_indices[split]]]\n",
    "        data_pkl[split_2gen][0] += data_pkl[split][0]\n",
    "        data_pkl[split_2gen][1] += data_pkl[split][1]\n",
    "        print(\"Split {}, data_pkl[split][0] len={}, data_pkl[split_2gen][0] len={}\".format(split, len(data_pkl[split][0]), len(data_pkl[split_2gen][0])))\n",
    "\n",
    "# data_npy[split_2gen] = np.concatenate((data_npy['front_raises134'], data_npy['deadlifting88'], data_npy['clean_jerk59'], data_npy['forcast254']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[134, 88, 59, 254].index(134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files and gen_train:\n",
    "    np.save(data_path['normal_train'], data_npy['normal_train'])\n",
    "    pickle.dump(data_pkl['normal_train'], open(label_path['normal_train'] ,\"wb\"))\n",
    "elif save_files:\n",
    "    np.save(data_path['normal_test'], data_npy['normal_test'])\n",
    "    pickle.dump(data_pkl['normal_test'], open(label_path['normal_test'] ,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_feeder_args = {'data_path': test_data_path, 'label_path': test_label_path}\n",
    "normal_train_feeder_args = {'data_path': data_path['normal_train'], 'label_path': label_path['normal_train']}\n",
    "normal_test_feeder_args = {'data_path': data_path['normal_test'], 'label_path': label_path['normal_test']}\n",
    "\n",
    "batch_size=32\n",
    "data_loader['normal_train'] = loader_initializer(normal_train_feeder_args, batch_size=batch_size, num_workers=0) # , drop_last=False)\n",
    "data_loader['normal_test' ] = loader_initializer(normal_test_feeder_args, batch_size=batch_size, num_workers=0) #, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = 'cuda:0'\n",
    "model = model_2ft.cuda()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loader = data_loader['normal_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing all but the new layer\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "    if \"fcn\" in name:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading files manually for verifying the loader\n",
    "# fdata = np.load(normal_train_feeder_args['data_path'], mmap_mode='r')\n",
    "# fsample_name, flabel = pickle.load(open(normal_train_feeder_args['label_path'], \"rb\"))\n",
    "\n",
    "# fdata = np.load(normal_test_feeder_args['data_path'], mmap_mode='r')\n",
    "# fsample_name, flabel = pickle.load(open(normal_test_feeder_args['label_path'], \"rb\"))\n",
    "\n",
    "# print(fdata.shape, len(fsample_name), len(flabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "for data, label in loader:\n",
    "            # get data\n",
    "            data = data.float().to(dev)\n",
    "            label = label.long().to(dev)\n",
    "\n",
    "            # forward\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, label)\n",
    "\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"Completed iteration\")\n",
    "            # statistics\n",
    "#             self.iter_info['loss'] = loss.data.item()\n",
    "#             self.iter_info['lr'] = '{:.6f}'.format(self.lr)\n",
    "#             loss_value.append(self.iter_info['loss'])\n",
    "#             self.show_iter_info()\n",
    "#             self.meta_info['iter'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
