{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:77% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:77% !important; }</style>\"))\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from net.st_gcn import Model\n",
    "from feeder.feeder import Feeder\n",
    "from ad_utils import map2ind, loader_initializer\n",
    "from torchlight.io import IO\n",
    "np.set_printoptions(linewidth=100, precision=5, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_args = {'layout':'openpose', 'strategy':'spatial'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading of original weights\n",
    "root_path = '/root/sharedfolder/Research/pose_ad/st-gcn/'\n",
    "io = IO(root_path)\n",
    "model_name = 'net.st_gcn.Model'\n",
    "model_args = {\n",
    "    'edge_importance_weighting':True,\n",
    "    'graph_args':graph_args,\n",
    "    'in_channels':3,\n",
    "    'num_class':400, }\n",
    "model_args_ft = {\n",
    "    'edge_importance_weighting':True,\n",
    "    'graph_args':graph_args,\n",
    "    'in_channels':3,\n",
    "    'num_class':4, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fio = io.load_model(model_name, **model_args)\n",
    "model_2ft = io.load_model(model_name, **model_args_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path =  os.path.join(root_path, 'models/kinetics-st_gcn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weights = torch.load(weights_path)\n",
    "weights_list = [[k.split('module.')[-1], v.cpu()] for k, v in weights.items()]\n",
    "weights = OrderedDict(weights_list)\n",
    "model_state_dict = model_2ft.state_dict()\n",
    "weights.pop('fcn.bias') # loading all but the Final FC layer's weight and bias\n",
    "weights.pop('fcn.weight')\n",
    "model_state_dict.update(weights)\n",
    "model_2ft.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_path = '/root/sharedfolder/datasets/data_ssd/kinetics-skeleton/st-gcn_kinetics/Kinetics/kinetics-skeleton/'\n",
    "train_data_path = os.path.join(data_dir_path, 'train_data.npy')\n",
    "train_label_path = os.path.join(data_dir_path, 'train_label.pkl')\n",
    "test_data_path = os.path.join(data_dir_path, 'val_data.npy')\n",
    "test_label_path = os.path.join(data_dir_path, 'val_label.pkl')\n",
    "\n",
    "test_feeder_args = {'data_path': test_data_path, 'label_path': test_label_path}\n",
    "train_feeder_args = {'data_path': train_data_path, 'label_path': train_label_path}\n",
    "\n",
    "batch_size=32\n",
    "data_loader = dict()\n",
    "data_loader['kinetics_train'] = loader_initializer(train_feeder_args, batch_size=batch_size)\n",
    "data_loader['kinetics_test' ] = loader_initializer(test_feeder_args, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_npy = np.load(train_data_path, mmap_mode='r')\n",
    "test_npy = np.load(test_data_path, mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_files = False\n",
    "gen_train = False\n",
    "if gen_train:\n",
    "    split_npy = train_npy\n",
    "    split_2gen = 'normal_train'\n",
    "    f = train_label_path\n",
    "else:\n",
    "    split_npy = test_npy\n",
    "    split_2gen = 'normal_test'\n",
    "    f = test_label_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_name, label = pickle.load(open(f, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = dict()\n",
    "label_path = dict()\n",
    "split_indices = dict()\n",
    "data_npy = dict()\n",
    "data_pkl = dict()  # An array of [sample_name, label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split front_raises134, data_pkl[split][0] len=50, data_pkl[split_2gen][0] len=50\n",
      "Split deadlifting88, data_pkl[split][0] len=49, data_pkl[split_2gen][0] len=99\n",
      "Split clean_jerk59, data_pkl[split][0] len=49, data_pkl[split_2gen][0] len=148\n",
      "Split forcast254, data_pkl[split][0] len=50, data_pkl[split_2gen][0] len=198\n"
     ]
    }
   ],
   "source": [
    "# Train Split Generator\n",
    "data_pkl['normal_train'] = [[],[]]  # Important to zero so not to accumulate older values\n",
    "data_pkl['normal_test'] = [[],[]]\n",
    "for split in ['front_raises134', 'deadlifting88', 'clean_jerk59', 'forcast254', 'normal_train', 'normal_test', 'abnormal_test', 'mixed_test']:\n",
    "    data_path[split]  =  os.path.join(data_dir_path, 'ad_experiment', split+'_data.npy')\n",
    "    label_path[split] =  os.path.join(data_dir_path, 'ad_experiment', split+'_label.pkl')\n",
    "    split_num = re.findall('\\d+', split)\n",
    "    if split_num != []:\n",
    "        split_indices[split] = [ind for ind, val in enumerate(label) if val == int(split_num[0])]\n",
    "        data_npy[split] = split_npy[split_indices[split]]\n",
    "        data_pkl[split] = [[sample_name[i] for i in split_indices[split]], [label[i] for i in split_indices[split]]]\n",
    "        data_pkl[split_2gen][0] += data_pkl[split][0]\n",
    "        data_pkl[split_2gen][1] += data_pkl[split][1]\n",
    "        print(\"Split {}, data_pkl[split][0] len={}, data_pkl[split_2gen][0] len={}\".format(split, len(data_pkl[split][0]), len(data_pkl[split_2gen][0])))\n",
    "\n",
    "data_npy[split_2gen] = np.concatenate((data_npy['front_raises134'], data_npy['deadlifting88'], data_npy['clean_jerk59'], data_npy['forcast254']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(198, 3, 300, 18, 2) 198\n"
     ]
    }
   ],
   "source": [
    "print(data_npy['normal_test'].shape, len(data_pkl['normal_test'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_files and gen_train:\n",
    "    np.save(data_path['normal_train'], data_npy['normal_train'])\n",
    "    pickle.dump(data_pkl['normal_train'], open(label_path['normal_train'] ,\"wb\"))\n",
    "elif save_files:\n",
    "    np.save(data_path['normal_test'], data_npy['normal_test'])\n",
    "    pickle.dump(data_pkl['normal_test'], open(label_path['normal_test'] ,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_feeder_args = {'data_path': test_data_path, 'label_path': test_label_path}\n",
    "normal_train_feeder_args = {'data_path': data_path['normal_train'], 'label_path': label_path['normal_train']}\n",
    "normal_test_feeder_args = {'data_path': data_path['normal_test'], 'label_path': label_path['normal_test']}\n",
    "mixed_test_feeder_args = {'data_path': data_path['mixed_test'], 'label_path': label_path['mixed_test']}\n",
    "\n",
    "batch_size=32\n",
    "data_loader['normal_train'] = loader_initializer(normal_train_feeder_args, batch_size=batch_size, num_workers=0)\n",
    "data_loader['normal_test' ] = loader_initializer(normal_test_feeder_args, batch_size=batch_size, num_workers=0)\n",
    "data_loader['mixed_test' ] = loader_initializer(mixed_test_feeder_args, batch_size=batch_size, suffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = 'cuda:0'\n",
    "model = model_2ft.cuda()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loader = data_loader['normal_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "st_gcn(\n",
       "  (gcn): ConvTemporalGraphical(\n",
       "    (conv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (tcn): Sequential(\n",
       "    (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(256, 256, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0))\n",
       "    (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Dropout(p=0, inplace)\n",
       "  )\n",
       "  (relu): ReLU(inplace)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.st_gcn_networks[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_bn.weight False\n",
      "data_bn.bias False\n",
      "st_gcn_networks.0.gcn.conv.weight False\n",
      "st_gcn_networks.0.gcn.conv.bias False\n",
      "st_gcn_networks.0.tcn.0.weight False\n",
      "st_gcn_networks.0.tcn.0.bias False\n",
      "st_gcn_networks.0.tcn.2.weight False\n",
      "st_gcn_networks.0.tcn.2.bias False\n",
      "st_gcn_networks.0.tcn.3.weight False\n",
      "st_gcn_networks.0.tcn.3.bias False\n",
      "st_gcn_networks.1.gcn.conv.weight False\n",
      "st_gcn_networks.1.gcn.conv.bias False\n",
      "st_gcn_networks.1.tcn.0.weight False\n",
      "st_gcn_networks.1.tcn.0.bias False\n",
      "st_gcn_networks.1.tcn.2.weight False\n",
      "st_gcn_networks.1.tcn.2.bias False\n",
      "st_gcn_networks.1.tcn.3.weight False\n",
      "st_gcn_networks.1.tcn.3.bias False\n",
      "st_gcn_networks.2.gcn.conv.weight False\n",
      "st_gcn_networks.2.gcn.conv.bias False\n",
      "st_gcn_networks.2.tcn.0.weight False\n",
      "st_gcn_networks.2.tcn.0.bias False\n",
      "st_gcn_networks.2.tcn.2.weight False\n",
      "st_gcn_networks.2.tcn.2.bias False\n",
      "st_gcn_networks.2.tcn.3.weight False\n",
      "st_gcn_networks.2.tcn.3.bias False\n",
      "st_gcn_networks.3.gcn.conv.weight False\n",
      "st_gcn_networks.3.gcn.conv.bias False\n",
      "st_gcn_networks.3.tcn.0.weight False\n",
      "st_gcn_networks.3.tcn.0.bias False\n",
      "st_gcn_networks.3.tcn.2.weight False\n",
      "st_gcn_networks.3.tcn.2.bias False\n",
      "st_gcn_networks.3.tcn.3.weight False\n",
      "st_gcn_networks.3.tcn.3.bias False\n",
      "st_gcn_networks.4.gcn.conv.weight False\n",
      "st_gcn_networks.4.gcn.conv.bias False\n",
      "st_gcn_networks.4.tcn.0.weight False\n",
      "st_gcn_networks.4.tcn.0.bias False\n",
      "st_gcn_networks.4.tcn.2.weight False\n",
      "st_gcn_networks.4.tcn.2.bias False\n",
      "st_gcn_networks.4.tcn.3.weight False\n",
      "st_gcn_networks.4.tcn.3.bias False\n",
      "st_gcn_networks.4.residual.0.weight False\n",
      "st_gcn_networks.4.residual.0.bias False\n",
      "st_gcn_networks.4.residual.1.weight False\n",
      "st_gcn_networks.4.residual.1.bias False\n",
      "st_gcn_networks.5.gcn.conv.weight False\n",
      "st_gcn_networks.5.gcn.conv.bias False\n",
      "st_gcn_networks.5.tcn.0.weight False\n",
      "st_gcn_networks.5.tcn.0.bias False\n",
      "st_gcn_networks.5.tcn.2.weight False\n",
      "st_gcn_networks.5.tcn.2.bias False\n",
      "st_gcn_networks.5.tcn.3.weight False\n",
      "st_gcn_networks.5.tcn.3.bias False\n",
      "st_gcn_networks.6.gcn.conv.weight False\n",
      "st_gcn_networks.6.gcn.conv.bias False\n",
      "st_gcn_networks.6.tcn.0.weight False\n",
      "st_gcn_networks.6.tcn.0.bias False\n",
      "st_gcn_networks.6.tcn.2.weight False\n",
      "st_gcn_networks.6.tcn.2.bias False\n",
      "st_gcn_networks.6.tcn.3.weight False\n",
      "st_gcn_networks.6.tcn.3.bias False\n",
      "st_gcn_networks.7.gcn.conv.weight False\n",
      "st_gcn_networks.7.gcn.conv.bias False\n",
      "st_gcn_networks.7.tcn.0.weight False\n",
      "st_gcn_networks.7.tcn.0.bias False\n",
      "st_gcn_networks.7.tcn.2.weight False\n",
      "st_gcn_networks.7.tcn.2.bias False\n",
      "st_gcn_networks.7.tcn.3.weight False\n",
      "st_gcn_networks.7.tcn.3.bias False\n",
      "st_gcn_networks.7.residual.0.weight False\n",
      "st_gcn_networks.7.residual.0.bias False\n",
      "st_gcn_networks.7.residual.1.weight False\n",
      "st_gcn_networks.7.residual.1.bias False\n",
      "st_gcn_networks.8.gcn.conv.weight False\n",
      "st_gcn_networks.8.gcn.conv.bias False\n",
      "st_gcn_networks.8.tcn.0.weight False\n",
      "st_gcn_networks.8.tcn.0.bias False\n",
      "st_gcn_networks.8.tcn.2.weight False\n",
      "st_gcn_networks.8.tcn.2.bias False\n",
      "st_gcn_networks.8.tcn.3.weight False\n",
      "st_gcn_networks.8.tcn.3.bias False\n",
      "st_gcn_networks.9.gcn.conv.weight False\n",
      "st_gcn_networks.9.gcn.conv.bias False\n",
      "st_gcn_networks.9.tcn.0.weight False\n",
      "st_gcn_networks.9.tcn.0.bias False\n",
      "st_gcn_networks.9.tcn.2.weight False\n",
      "st_gcn_networks.9.tcn.2.bias False\n",
      "st_gcn_networks.9.tcn.3.weight False\n",
      "st_gcn_networks.9.tcn.3.bias False\n",
      "edge_importance.0 False\n",
      "edge_importance.1 False\n",
      "edge_importance.2 False\n",
      "edge_importance.3 False\n",
      "edge_importance.4 False\n",
      "edge_importance.5 False\n",
      "edge_importance.6 False\n",
      "edge_importance.7 False\n",
      "edge_importance.8 False\n",
      "edge_importance.9 False\n",
      "fcn.weight True\n",
      "fcn.bias True\n"
     ]
    }
   ],
   "source": [
    "# Freezing all but the new layer\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "    if \"fcn\" in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Opens st-gcn layer 9 for learning\n",
    "# model.st_gcn_networks[9].requires_grad = True\n",
    "for name, param in model.named_parameters():\n",
    "#     if name.find('9') != -1:\n",
    "#         param.requires_grad = True\n",
    "    print (name, param.requires_grad) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198 198\n"
     ]
    }
   ],
   "source": [
    "# # loading files manually for verifying the loader\n",
    "# fdata = np.load(normal_train_feeder_args['data_path'], mmap_mode='r')\n",
    "# fsample_name, flabel = pickle.load(open(normal_train_feeder_args['label_path'], \"rb\"))\n",
    "\n",
    "# fdata = np.load(normal_test_feeder_args['data_path'], mmap_mode='r')\n",
    "fsample_name, flabel = pickle.load(open(normal_test_feeder_args['label_path'], \"rb\"))\n",
    "\n",
    "print(len(fsample_name), len(flabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check - Using the original data \n",
    "# model = model_fio.cuda()\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# loader = data_loader['kinetics_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "# optimizer = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 (Epoch: 0), loss is 0.2892225980758667\n",
      "Iteration 10 (Epoch: 0), loss is 0.24911516904830933\n",
      "Met loss 0.12788908183574677 in iteration 13\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "for epoch in range(2):\n",
    "    for itern, [data, label] in enumerate(loader):\n",
    "            # get data\n",
    "            data = data.float().to(dev)\n",
    "            label = torch.from_numpy(map2ind(label))\n",
    "            label = label.long().to(dev)\n",
    "\n",
    "            # forward\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, label)\n",
    "\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if itern%10 == 0: \n",
    "                print(\"Iteration {} (Epoch: {}), loss is {}\".format(itern, epoch, loss))\n",
    "            if loss < 0.15:\n",
    "                print(\"Met loss {} in Epoch {}, iteration {}\".format(loss, epoch, itern))\n",
    "                break\n",
    "    else:\n",
    "        continue\n",
    "    break\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0', dtype=torch.uint8)\n",
      "[[99.28692  0.34906  0.16467  0.19935]\n",
      " [ 2.62818  1.21594  0.54397 95.61191]\n",
      " [98.52151  0.49589  0.71857  0.26402]\n",
      " [ 0.6491   0.09719 99.24773  0.00597]\n",
      " [ 7.56669  2.93043  4.22065 85.28223]\n",
      " [16.08512  9.81683  7.16591 66.93213]\n",
      " [ 3.24515  0.43408  0.33628 95.98448]\n",
      " [ 4.86326  3.26681  2.09784 89.77209]\n",
      " [ 0.99868 96.28539  2.68779  0.02814]\n",
      " [ 4.33689 78.13078 17.17232  0.36001]\n",
      " [ 8.10341 60.57731 31.21624  0.10304]\n",
      " [ 5.69339  5.16186  3.32176 85.82299]\n",
      " [ 0.06614  0.01862 99.89435  0.0209 ]\n",
      " [ 5.92181  6.13845  3.27562 84.66412]\n",
      " [97.62436  0.29696  0.12541  1.95328]\n",
      " [21.95343 64.76116 12.34718  0.93823]\n",
      " [ 4.93856  2.50877 92.4697   0.08298]\n",
      " [10.85381  0.87585  1.79512 86.47523]\n",
      " [13.34172 79.11286  7.29128  0.25415]\n",
      " [98.15436  0.48274  0.99597  0.36692]\n",
      " [ 4.62681  9.60966 83.60063  2.1629 ]\n",
      " [ 0.60818  0.16174  0.20115 99.02893]\n",
      " [ 9.90487  2.5043   2.31772 85.27312]\n",
      " [93.41393  1.96221  4.46714  0.15672]\n",
      " [ 7.58823 89.33777  2.6271   0.4469 ]\n",
      " [ 2.07988  1.84665 95.77646  0.29701]\n",
      " [ 4.65976  5.526    3.76935 86.0449 ]\n",
      " [ 3.49154 76.67871 19.18625  0.64349]\n",
      " [86.93197  7.45454  3.97571  1.63778]\n",
      " [ 2.89766  3.58159 93.50858  0.01217]\n",
      " [ 1.04413  0.61606  0.40384 97.93597]\n",
      " [ 3.0001   1.93748 95.02985  0.03257]] \n",
      " tensor([0.9929, 0.9561, 0.9852, 0.9925, 0.8528, 0.6693, 0.9598, 0.8977, 0.9629,\n",
      "        0.7813, 0.6058, 0.8582, 0.9989, 0.8466, 0.9762, 0.6476, 0.9247, 0.8648,\n",
      "        0.7911, 0.9815, 0.8360, 0.9903, 0.8527, 0.9341, 0.8934, 0.9578, 0.8604,\n",
      "        0.7668, 0.8693, 0.9351, 0.9794, 0.9503],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.8866, device='cuda:0', grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfmax_vals, cls = torch.max(output, 1)\n",
    "print(cls==label)\n",
    "softmax = nn.Softmax(dim=1)\n",
    "osoftmax_gpu = softmax(output)\n",
    "\n",
    "sfmax_vals, cls = torch.max(osoftmax_gpu, 1)\n",
    "\n",
    "osoftmax = 100 * osoftmax_gpu.cpu().detach().numpy()\n",
    "print(osoftmax, '\\n', sfmax_vals)\n",
    "torch.mean(sfmax_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loop\n",
    "model.eval()\n",
    "loader = data_loader['mixed_test']\n",
    "loss_value = []\n",
    "result_frag = []\n",
    "label_frag = []\n",
    "epoch_info = dict()\n",
    "confusion_mat = np.zeros([5,5], dtype=np.int32)\n",
    "evaluation = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tt_data = np.load(data_path['mixed_test'])\n",
    "# fpkl = '/root/sharedfolder/datasets/data_ssd/kinetics-skeleton/st-gcn_kinetics/Kinetics/kinetics-skeleton/ad_experiment/mixed_test_label.pkl'\n",
    "# fpkl_names, fpkl_labels = pickle.load(open(fpkl ,\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 300, 18, 2]) 32\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2])\n",
      "torch.Size([32, 3, 300, 18, 2]) 32\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "torch.Size([32, 3, 300, 18, 2]) 32\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n",
      "torch.Size([32, 3, 300, 18, 2]) 32\n",
      "tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "torch.Size([32, 3, 300, 18, 2]) 32\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3])\n",
      "torch.Size([32, 3, 300, 18, 2]) 32\n",
      "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3])\n",
      "torch.Size([32, 3, 300, 18, 2]) 32\n",
      "tensor([3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4])\n",
      "torch.Size([32, 3, 300, 18, 2]) 32\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4])\n",
      "torch.Size([32, 3, 300, 18, 2]) 32\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4])\n",
      "torch.Size([32, 3, 300, 18, 2]) 32\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4])\n",
      "torch.Size([32, 3, 300, 18, 2]) 32\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4])\n",
      "torch.Size([32, 3, 300, 18, 2]) 32\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4])\n",
      "torch.Size([32, 3, 300, 18, 2]) 32\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4])\n",
      "torch.Size([32, 3, 300, 18, 2]) 32\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4])\n",
      "torch.Size([32, 3, 300, 18, 2]) 32\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4])\n",
      "torch.Size([32, 3, 300, 18, 2]) 32\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4])\n",
      "torch.Size([32, 3, 300, 18, 2]) 32\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4])\n",
      "torch.Size([32, 3, 300, 18, 2]) 32\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4])\n",
      "torch.Size([32, 3, 300, 18, 2]) 32\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4])\n",
      "torch.Size([30, 3, 300, 18, 2]) 30\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "for itern, [data, label] in enumerate(loader):\n",
    "    print (data.shape, len(label))\n",
    "    data = data.float().cuda()\n",
    "    label = torch.from_numpy(map2ind(label, from_arr=[59, 88, 134, 254], to_arr=None, def_val=4))\n",
    "    print(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 300, 18, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorCopy.cpp:20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-9ddbbfb46b5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# get data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mlabel_mapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap2ind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_arr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m59\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m88\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m134\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m254\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_arr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdef_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_mapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorCopy.cpp:20"
     ]
    }
   ],
   "source": [
    "for itern, [data, label] in enumerate(loader):\n",
    "    # get data\n",
    "    print(data.shape)\n",
    "    data = data.float().cuda()\n",
    "    label_mapped = map2ind(label, from_arr=[59, 88, 134, 254], to_arr=None, def_val=4)\n",
    "    label = torch.from_numpy(label_mapped)\n",
    "    label = label.long().cuda()\n",
    "\n",
    "    # inference\n",
    "    with torch.no_grad():\n",
    "        output = model(data)\n",
    "    result_frag.append(output.data.cpu().numpy())\n",
    "\n",
    "    # get loss\n",
    "    if evaluation:\n",
    "        loss = loss_fn(output, label)\n",
    "#         confusion_mat[label_mapped, np.argmax(output.cpu(), axis=1)] += 1\n",
    "        loss_value.append(loss.item())\n",
    "        label_frag.append(label.data.cpu().numpy())\n",
    "\n",
    "result = np.concatenate(result_frag)\n",
    "# np.savetxt(\"kinetics400_confusion.csv\", confusion_mat)\n",
    "if evaluation:\n",
    "    label2 = np.concatenate(label_frag)\n",
    "    epoch_info['mean_loss'] = np.mean(loss_value)\n",
    "#     show_epoch_info()\n",
    "\n",
    "    # show top-k accuracy\n",
    "#     for k in self.arg.show_topk:\n",
    "#         self.show_topk(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 512.85,
   "position": {
    "height": "40px",
    "left": "1508px",
    "right": "16px",
    "top": "3px",
    "width": "396px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
